---
layout: post
title: "英伟达又放AI大招"
date: 2022-03-25 09:57:52 +0800
categories: zhongguodianzibao
tags: 中国电子报新闻
---
<p>3月22日晚11点，英伟达CEO黄仁勋在GTC大会上又演讲了，演讲地点从自家厨房搬到了公司。</p>
 <p>此次演讲，老黄将关注的重点聚焦在AI上。100分钟的演讲共提及161次AI，从英伟达当前支持的AI应用，到更支持AI技术实现的处理器，再到英伟达提供的AI平台Omniverse.看来老黄这次是打算跟AI死磕了。</p>
 <p>天气预报AI模型提前一周预测灾难性天气</p>
 <p>“传统的数值模拟需要一年的时间，而现在只需要几分钟。”黄仁勋介绍称，英伟达与包括加州理工学院、伯克利实验室在内的多家科研机构合作开发的FourCastNet天气预报AI模型，将能够预测飓风、极端降水等天气事件。黄仁勋称，FourCastNet由傅里叶神经算子提供动力支持，基于10TB的地球系统数据进行训练。依托这些数据，以及NVIDIA Modulus和Omniverse，可实现提前一周预测灾难性极端降水的精确路线。</p>
 <p>不仅是在极端天气愈加频繁的情况下发挥作用，英伟达的产品也使因疫情而愈加普遍化的在线办公更加智能化。配合在线会议的发展，黄仁勋在演讲中正式发布NVIDIA Riva.这是一种先进且基于深度学习的端到端语音AI，可以自定义调整优化，已经过预训练，客户可以使用定制数据进行优化，使其学习特定话术，以应对不同行业、国家和地区的需求。</p>
 <p>另一种为应对在线办公而生的SDK(Software Development Kit，软件开发工具包)Maxine，也在黄仁勋此次视频演讲中呈现。这是一个AI 模型工具包，目前已拥有30个模型，可以帮助用户在参与线上会议的时候与所有人保持眼神交流，即便是正在读稿也不会被发现，还能实现语言之间的实时翻译。</p>
 <p>“搭积木”技术建成AI工厂</p>
 <p>“这是全球AI计算基础架构引擎的巨大飞跃，下面隆重推出NVIDIA H100.”在演讲中，黄仁勋再次推出新产品。H100采用TSMC 4N工艺，具有800亿个晶体管，是首款支持PCIe 5.0标准的GPU，也是首款采用HBM3标准的GPU，单个H100可支持40Tb/s的IO带宽。从另一个角度来说，20块H100 GPU便可承托相当于全球互联网的流量。Hopper架构相较于前一代Ampere架构实现了巨大飞跃，其算力达到4PetaFLOPS的FP8,2PetaFLOPS的FP16,1PetaFLOPS的TF32,60TeraFLOPS的FP64和FP32.H100采用风冷和液冷设计，据黄仁勋介绍，这是首个实现性能扩展至700瓦的GPU.在AI处理方面，Hopper H100 FP8的4PetaFLOPS算力是Ampere A100 FP16的6倍。</p>
 <p>不仅注重速度和算力，H100也注重数据使用的安全性。</p>
 <p>“通常，敏感数据处于静态和在网络中传输时会进行加密，但在使用期间却不受保护。”黄仁勋假设了一个场景，若一家公司具有价值数百万美元的AI模型，而在使用期间不受保护，则该公司将面临巨大的数据风险。他声称，Hopper机密计算能够保护正在使用的数据和应用，能够保护所有者的AI模型和算法的机密性以及完整性。此外，软件开发者和服务提供商可在共享或远程基础架构上分发和部署宝贵的专有AI模型，在保护其知识产权的同时扩展业务模式。</p>
 <p>黄仁勋隆重发布的全新AI计算系统DGX H100展现出英伟达像搭积木一样拓展处理器性能的技术。借助NVLink连接，DGX使8块H100成为了一个巨型GPU:拥有6400亿个晶体管，具备32PetaFLOPS的AI性能，具有640GB HBM3显存以及24TB/s的显存带宽。</p>
 <p>仅仅连接GPU还不过瘾，英伟达“搭积木”的技术可以再将8块GPU组成的DGX进行连接。黄仁勋推出NVIDIA NVLink Switch系统，借助NVLink Switch系统，计算系统可扩展为一个巨大的拥有32个节点、256个GPU的DGX POD， HBM3显存高达20.5TB，显存带宽高达768TB/s.每个DGX都可借助4端口光学收发器连接到NVLink Switch，每个端口都有8个100G-PAM4通道，每秒能够传输100GB数据，32个NVLink 收发器可连接到1个机架单元的NVLinkSwitch系统，以此实现超强的拓展性。</p>
 <p>黄仁勋称，英伟达正在建造EOS——英伟达打造的首个Hopper AI工厂，搭载18个DGX POD、576台DGX、4608个H100 GPU.在传统的科学计算领域，EOS的速度是275PetaFLOPS，比A100驱动的美国速度最快的科学计算机Summit还快1.4倍。在AI方面， EOS的AI处理速度是18.4ExaFLOPS，比全球最大的超级计算机——日本的Fugaku快4倍。</p>
 <p>从H100到使用8块H100构成的AI计算系统DGX H100，再到使用256个GPU的DGX POD，以至于HopperAI工厂，英伟达像搭积木一样，构建起一套辅助AI计算的硬件系统。</p>
 <p>与英特尔打擂台的Grace有望明年供货</p>
 <p>在去年的GTC大会上，英伟达推出了首颗数据中心CPU——Grace.按照英伟达的介绍，这是一颗高度专用型处理器，主要面向大型数据密集型HPC和AI应用。与英特尔CPU坚守的x86架构不同，Grace另起炉灶采用ARM架构。黄仁勋声称，服务器用上这款CPU后， AI性能将超过x86架构CPU的10倍。</p>
 <p>此次GTC大会，黄仁勋称Grace进展飞速，有望明年供货。不止于此，他们将“搭积木”技术继续应用在了Grace技术上。通过Grace与Hopper连接，英伟达打造了单一超级芯片模组Grace-Hopper.黄仁勋称Grace-Hopper的关键驱动技术之一是内存一致性芯片之间的NVLink互连，每个链路的速度达900GB/s.Grace CPU也可以是由两个通过芯片之间的NVLink连接、保证一致性的CPU芯片组成的超级芯片，可拥有144个CPU核心，内存带宽高达1TB/s。</p>
 <p>接着，老黄给出了Grace和Hopper能够打造的不同排列组合方案：2个Grace CPU组成的超级芯片、1个Grace加1个Hopper组成的超级芯片、1个Grace加2个Hopper 的超级芯片、搭载2个Grace和2个Hopper的系统、2个Grace加4个Hopper组成的系统、2个Grace加8个Hopper组成的系统等。</p>
 <p>“老黄”与“小黄”的对话透露何种玄机</p>
 <p>老黄的这次发布会，再次请出了英伟达仿照自己的形象设计的虚拟人——Toy Jensen.而这次，虚拟人Toy Jensen出现的主要目的，是展示英伟达用于构建虚拟形象或数字人框架的Omniverse Avatar。</p>
 <p>Toy Jensen在完成了一轮百科功能展示之后，又兴致勃勃地站在老黄对面展示起了自己的出生地——Omniverse Avatar.这是一个基于Omniverse平台构建的框架，用户可以快速构建和部署虚拟形象。“小黄”Toy Jensen的声音、面部均由英伟达的系列工具提供。“小黄”的声音由Riva的文本转语音RADTTS合成，Omniverse的动画图形可定义并控制其动作，Omniverse Audio2Face可驱动其面部动画。NVIDIA的开源材质定义语言(MDL) 可增加触感，使“小黄”的衣服看起来更有合成皮革的视觉感受，而不仅仅是塑料。最终，“小黄”的形象通过RTX渲染器能以实时高保真的程度呈现。得益于Riva中的最新对话式AI技术和Megatron 530B NLP模型，“小黄”可以与真人进行对话。不仅如此，归功于一款使用Omniverse Avatar构建的应用Tokkio，“小黄”还能连接到更多类型的数据，它将客户服务AI引入零售店快餐餐厅，甚至网络。</p><p class="em_media">（文章来源：中国电子报）</p>

<http://finews.zning.xyz/html_News/NewsShare.html?infoCode=NW202203252322825492>

[返回中国电子报新闻](//finews.withounder.com/category/zhongguodianzibao.html)｜[返回首页](//finews.withounder.com/)